# NLP Feature Extraction and Vocabulary

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Jupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?logo=Jupyter)](https://jupyter.org/try)

A comprehensive guide to understanding how text is converted into numerical features for machine learning models.

## ðŸ“‹ Table of Contents

- [ðŸ“– Overview](#overview)
- [â“ Why Feature Extraction is Needed](#why-feature-extraction-is-needed)
- [ðŸ“š Understanding Vocabulary](#understanding-vocabulary)
- [ðŸŽ’ Bag of Words (BoW) Model](#bag-of-words-bow-model)
- [âš ï¸ The Sparse Vector Problem](#the-sparse-vector-problem)
- [âš¡ Performance Issues](#performance-issues)
- [ðŸ”§ Better Alternatives](#better-alternatives)
- [ðŸš€ Getting Started](#getting-started)
- [ðŸ“„ License](#license)

## ðŸ“– Overview

This repository explores fundamental concepts in Natural Language Processing (NLP), specifically focusing on how raw text data is transformed into numerical representations that machine learning models can understand and process.

## â“ Why Feature Extraction is Needed

Machine learning models can't understand words or text directly â€” they only understand numbers. So we must convert text into numerical form before we can train a model.

That conversion process is called **Feature Extraction** (or sometimes **Vectorization**).

## ðŸ“š Understanding Vocabulary

The vocabulary is simply the list of all unique words that appear in your training data.

### Example:

If your dataset has these two tweets:

- "I am happy because I am learning NLP"
- "NLP is fun"

Then your vocabulary (the list of all unique words) is:

```
[I, am, happy, because, learning, NLP, is, fun]
```

So the vocabulary size (V) = 8.

## ðŸŽ’ Bag of Words (BoW) Model

Each tweet (sentence) can now be represented as a vector of size V (one position for each word in the vocabulary).

For each position:

- Put **1** if the word appears in the tweet
- Put **0** if it does not

### Example 1

**Tweet:** "I am happy because I am learning NLP"

| Word     | Present? |
| -------- | -------- |
| I        | 1        |
| am       | 1        |
| happy    | 1        |
| because  | 1        |
| learning | 1        |
| NLP      | 1        |
| is       | 0        |
| fun      | 0        |

**Vector:** `[1, 1, 1, 1, 1, 1, 0, 0]`

### Example 2

**Tweet:** "NLP is fun"

| Word     | Present? |
| -------- | -------- |
| I        | 0        |
| am       | 0        |
| happy    | 0        |
| because  | 0        |
| learning | 0        |
| NLP      | 1        |
| is       | 1        |
| fun      | 1        |

**Vector:** `[0, 0, 0, 0, 0, 1, 1, 1]`

## âš ï¸ The Sparse Vector Problem

When you have a small vocabulary (like 8 words), this is fine. But imagine analyzing millions of tweets â€” your vocabulary could have 50,000 or even 1,000,000 unique words!

Then each sentence becomes a very large vector, like this:

```
[0, 0, 0, 1, 0, 0, â€¦, 0, 1, 0, â€¦] â† thousands of zeros!
```

This is called a **sparse vector** â€” meaning most values are 0 (the word isn't there), and only a few are 1 (the word appears).

## âš¡ Performance Issues

### ðŸ”¢ High Dimensionality

- More words = bigger vectors = more features to process
- Example: If V = 100,000 words, you must train 100,000 parameters (weights, Î¸)

### ðŸ’¾ More Memory Usage

- You must store huge vectors full of mostly zeros

### â±ï¸ Longer Training Time

- The computer must process more numbers and update more parameters

### ðŸŒ Longer Prediction Time

- Even after training, predictions take longer because the input vectors are large

### âš™ï¸ Parameter Complexity

In supervised learning, your model learns one parameter (Î¸) for each feature (word).

- If vocabulary size is V â†’ model must learn V parameters (Î¸â‚, Î¸â‚‚, â€¦, Î¸V)
- More features â†’ more parameters â†’ slower and heavier model

## ðŸ”§ Better Alternatives

To reduce these issues, we use more compact feature extraction methods:

### ðŸ“Š TF-IDF

Weighs important words higher and ignores common ones.

### ðŸ§  Word Embeddings

- **Word2Vec, GloVe, BERT**
- Represent each word in fewer dimensions (like 100 or 300 instead of 100,000)
- Capture semantic meaning

These methods make the vectors smaller, denser, and faster to train.

## ðŸš€ Getting Started

### Prerequisites

- Python 3.8+
- NumPy
- Pandas
- Scikit-learn
- Jupyter Notebook

### Installation

1. Clone this repository:

```bash
git clone https://github.com/yourusername/nlp-learning-notebooks.git
cd nlp-learning-notebooks/vocabulary-feature-extraction
```

2. Install required packages:

```bash
pip install numpy pandas scikit-learn jupyter matplotlib
```

3. Launch Jupyter Notebook:

```bash
jupyter notebook feature-extraction-vocabulary.ipynb
```

### Quick Start

To implement these concepts in your NLP projects, consider starting with scikit-learn's feature extraction tools:

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Basic Bag of Words
vectorizer = CountVectorizer()
bow_features = vectorizer.fit_transform(documents)

# TF-IDF
tfidf_vectorizer = TfidfVectorizer()
tfidf_features = tfidf_vectorizer.fit_transform(documents)
```

## ðŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

> **Note**: Given a tweet, or some text, you can represent it as a vector of dimension V, where V corresponds to your vocabulary size. If you had the tweet "I am happy because I am learning NLP", then you would put a 1 in the corresponding index for any word in the tweet, and a 0 otherwise. As you can see, as V gets larger, the vector becomes more sparse. Furthermore, we end up having many more features and end up training Î¸ parameters. This could result in larger training time, and large prediction time.
